jobs:
## TEST
- job: Test
  displayName: Test
  dependsOn: []
  continueOnError: false
  pool:
    vmImage: 'ubuntu-20.04'
  workspace:
    clean: all

  steps:
  # set conda-forge repository
  - task: Bash@3
    displayName: 'Set conda-forge repository'
    inputs:
      targetType: inline
      script: |
        set -euo pipefail
        eval "$(conda shell.bash hook)"
        conda config --remove channels defaults
        conda config --append channels conda-forge
  # get Databricks token from Key Vault
  - task: AzureKeyVault@1
    displayName: 'Azure Key Vault: get Databricks token'
    inputs:
      azureSubscription: $(SERVICE_CONNECTION_NAME)
      KeyVaultName: '$(KEYVAULT_NAME)'
      SecretsFilter: 'unit-dbx-token'
  # environment initialization
  - task: Bash@3
    displayName: 'Python environment initialization'
    inputs:
      targetType: inline
      script: |
        set -euo pipefail
        cp .env.dist .env
        sed -i 's/DBX_TOKEN=/DBX_TOKEN=$(unit-dbx-token)/g' .env
        export SHELL=$SHELL
        ./env-init.sh -y --verbose
  # check coding standards
  - task: Bash@3
    displayName: 'Testing coding standards'
    inputs:
      targetType: inline
      script: |
        set -euo pipefail
        eval "$(conda shell.bash hook)"
        conda activate $PWD/.venv
        source $HOME/.poetry/env
        poe black-check
        poe flake8
  # run Container tests
  - task: Bash@3
    displayName: 'Running container tests'
    inputs:
      targetType: inline
      script: |
        set -euo pipefail
        eval "$(conda shell.bash hook)"
        conda activate $PWD/.venv
        ~/.poetry/bin/poetry install --no-root --no-dev # remove all dev dependencies
        pip install databricks-connect==7.3.7 # pyspark is still needed
        python src/__myproject__/ContainerTest.py